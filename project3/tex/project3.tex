\documentclass[12pt]{extarticle}
\usepackage[english]{babel}
\usepackage{NotesTeX}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{listings}
\usepackage{extarrows}
\usepackage{parskip}
\usepackage{eurosym}
\usepackage{footmisc}
\def\code#1{\texttt{#1}}

\graphicspath{{../output/}}
\collaborationImg{\includegraphics[width=30mm]{../../pictures/UIO.png}}

\author{\Large Vetle Nevland, Vetle Vikenes \& Sigurd Sørlie Rustad}
\title{\Huge Machine Learning: Bra Tittel}
\affiliation{\large FYS-STK4155 – Applied Data Analysis and Machine Learning
\\Autumn 2021\\Department of Physics\\University of Oslo\\\\\today}
\begin{document}
\abstract{This project investigates two numerical techniques for solving differential equations, forward euler and neural network. Conventional finite difference methods are efficient and accurate for solving not too intricate differential equations. Stability is an issue though, particularly for explicit schemes such as forward euler. Results from numerical simulations suggest that the output of a neural network is able to converge to the true solution. In contrast to forward euler, neural network is more or less unaffected by the time step. Instead, hyperparameters such as learning rate and epochs determine the accuracy and convergence property. Neural networks have their limitation of an extensive learning process, but offer a flexible and robust alternative to finite difference methods that is presumeably independent of temporal discretization. This opens for new possibilities in the field of differential equations. Our results strongly suggest that neural networks are well suited for simulating differential equations over long time periods, being much less sensitive of small time steps which would increase efficiency and accuracy simultaneously.}
\maketitle
\pagestyle{myplain}


\section{Introduction}

We will in no way answer all questions linked to the aforementioned methods. So that anyone can reproduce or continue our studies, we list all the code, results and instructions on running the code in our GitHub repository\footnote{\href{https://github.com/sigurdru/FYS-STK4155/tree/main/project2}{https://github.com/sigurdru/FYS-STK4155/tree/main/project3}}.

Neural network is a trending machine learning method due to its ability to solve a wide range of problems. By calculating the error at the output layer with an appropriate loss function, the weights and biases are updated accordingly to provide the best possible approximation to the true solution of the problem. The inner workings a neural network is considered a black box, difficult to justify theoretically, but its flexibility and wide applicability makes it a relevant method for approaching more unconventional tasks. This includes solving differential equations, which will be the main purpose for this project. 

The benefit of solving differential equations with neural network is that we don't need to know the analytical solution. By starting with a suitable guess of a solution, the neural network will converge towards the analytical solution given an appropriate loss function that initiaites gradient descent. Hence, neural networks are flexible enough to solve complex differential equations, including ODEs and PDEs. The focus in this project will be on solving a one dimensional diffusion equation. This is a PDE with a rather intricate analytical solution. It is of interest to investigate the potential for solving this PDE with a neural network.

A natural extension is to consider another differential equation with another applicablility. We will consider a first order ODE whose solution is the eigenvector corresponding to the largest eigenvalue of a real, symmetric matrix. Solving this differential equation will reveal the potential for a neural network to find eigenvalues of symmetric matrices. The result will be compared with traditional numerical methods for solving differential equations, particularly the forward euler method. A critical discussion of the methods will then be presented with focus on flexibility and computational efficiency. An overarching question considered is if neural networks can compete with state-of-the-art numerical methods for solving differential equations.


\section{Theory}
In the theory-section we aim to give a brief explanation of the main concepts and terminology used in this report. 

\subsection{The diffusion equation}

The full diffusion equation reads
\begin{equation}
\frac{\partial u(\mathbf{r}, t)}{\partial t} = \nabla \cdot \left[D(u, \mathbf{r})\nabla u(\mathbf{r}, t)\right],
\end{equation}
where $\mathbf{r}$ is a positional vector and $D(u,\mathbf{r})$ the collective diffusion coefficient. If $D(u,\mathbf{r}) = 1$ the equation simplifies to a linear differential equation
\begin{equation}
\frac{\partial u}{\partial t} = \nabla^2u(\mathbf{r}, t),
\end{equation}

In this report we are going to study a one dimensional rod of length $L=1$. I.e. we need the one dimensional diffusion equation
\begin{align}
\label{eq:diffusion_equation_1D}
\frac{\partial^2 u(x,t)}{\partial x^2} &= \frac{\partial u(x,t)}{\partial t},
\end{align}
with boundary conditions
\begin{align}
u(x,0) &= \sin(\pi x) \ \ 0\leq x\leq L,\\
u(0,t) &= 0 \ \ t\geq 0 \text{ and} \\
u(L,t) &= 0 \ \ t\geq 0.
\end{align}

\subsection{Analytical solution}


An analytical solution of the 1D diffusion equation can be dervied using the method of separation of variables.
For a complete derivation, please refer to the Appendix \ref{app:1D_diff_eq}.
The analytical solution satisfying the given initial and boundary conditions is

\begin{align} \label{eq:analytical_solution_diffusion}
	u(x,t) = e^{-\pi^2 t} \sin(\pi x)
\end{align}

\subsection{Explicit forward Euler}
In this section we want to cover the explicit forward Euler. By explicit we mean that the value at the next grid point is determined entirely by known or previously calculated values.

To approximate the solution of equation \eqref{eq:diffusion_equation_1D}, we have to discretize the position and time coordinates. We can choose  $\Delta x = L/N$ and $\Delta t$ as small steps in $x$-direction and time, respectively, where $N$ are the number of discretized points in $x$-direction. Then we can define the value domain of $t$ and $x$,
\begin{equation}
t_j = j\Delta t, \ \ j\in \mathbb{N}_0 \ \ \wedge \ \ x_i = i\Delta x, \ \ \{i \in \mathbb{N}_0 | i \leq N\}.
\end{equation}

The algorithm for explicit forward Euler in one dimension (from \cite{lectures2015} chapter 10.2.1) reads
\begin{equation}
\label{eq:forward_euler}
u_{i, j+1} = \alpha\, u_{i-1, j} + (1 - 2\alpha) u_{i,j} + \alpha\, u_{i+1, j}
\end{equation}
where
\begin{equation}
\alpha = \frac{\Delta t}{\Delta x^2}.
\end{equation}
This has a local approximate error of $O(\Delta t)$ and $O(\Delta x ^2)$. Experiments show that the following bound on $\alpha$ ensures stability.
\begin{equation}
	\label{eq:stability}
	\alpha \le \frac{1}{2}.
\end{equation} 

Even though $\alpha = 0.5$ is at the transition between stable and unstable solutions, experiments show that it is a safe choice for the diffusion equation that produces stable solutions \cite{Linge2017}. 

\subsection{Solving Differential equations with deep learning}
Many of the concepts we use in this section are covered in a previous report \cite{project2}. There, in the theory section, we cover central concepts like deep neural networks, cost functions, gradient descent, etc. In this report we will follow closely the work by Maziar Raissi et.al. \cite{raissi2017physics} and this Jupyter-Notebook \footnote{\texttt{https://colab.research.google.com/github/janblechschmidt/PDEsByNNs/blob/main/PINN \_Solver \textcolor{red}{Sigurd: Legg jupyter som referanse, fjern footnote}.ipynb\#scrollTo=1PlqQM9aZEkd}}. This method works with any partial differential equation (PDE), however we limit ourselves to the diffusion equation in one dimension \eqref{eq:diffusion_equation_1D} which we are going to study in this report. 

The idea is that we have some neural network $N$ with parameters $\theta$, including $x$ and $t$ as inputs, which returns $u_{\theta}(x,t)$. After training the neural network, one obtains the parameters $\hat{\theta}$ that best approximates its output with the actual solution, $u(x,t)$, of the PDE
\begin{align}
	u_{\hat{\theta}} (x, t) \approx u(x, t).
\end{align}

The process starts by defining a trial function $u_{\theta}(x,t)$ which is an initial guess of the actual solution $u(x,y)$, defined as

\begin{align}
	u_{\theta}(x,t) = u_0(x, t) + f\big(x, N(x,\theta)\big),
	\label{eq:NN_model}
\end{align}
where $u_0$ is a function specifically designed to satisfy the initial and boundary conditions of the PDE. The function $f$ is the part that is to be optimized by the algorithm. It depends on $x$ and the parameters $\theta$ through the neural network $N$, and should return zero for the initial and boundary conditions. Constructing the function $f$ should beneficially incorporate prior knowledge about how the solution behaves, because this will accelerate the learning process.

\par We then define the residual $r_\theta(t, x)$, which is the output of the neural network inserted back into the PDE \eqref{eq:diffusion_equation_1D}
\begin{align}
	r_\theta(x, t) \equiv \frac{\partial u_\theta}{\partial t} - \frac{\partial^2 u_\theta}{\partial x^2}.
	\label{eq:residual}
\end{align}

Ideally this should be zero, implying a perfect reproduction of the actual solution. The residual becomes our main object for training, as it gives a measure of how good the neural network approximates the actual solution. For the neural network to improve its predictions a loss function is required, which is to minimized. For this we use the mean sum of squared residuals, including the initial and boundary points, across all data samples $n$ 

\begin{align}
	L(x, t, \theta) = \frac{1}{n} \sum_{i=1}^n [r_\theta(x_i,t_i)]^2 + \frac{1}{n_B}\sum_{i=1}^{n_B} (u_{\theta}(x_b,t_i) - u(x_b,t_i))^2 + \frac{1}{n_I} \sum_{i=1}^{n_I} (u_{\theta}(x_i,0) - u(x_i,0))^2.
	\label{eq:NN_loss}
\end{align}
Note, because we know the initial and boundary conditions we include them in the loss function, and they will act as regularization terms. $n_B$ and $n_I$ is the number of training points on the boundary and the initial time step, respectively. The boundary points are denoted by $x_b$.
Optimization is then done through a form of gradient descent, minimizing the loss with respect to the parameters $\theta$

\begin{equation}
	\theta \leftarrow \theta - \eta \nabla_{\theta}L(x,t,\theta),
\end{equation}

where $\eta$ is the learning rate, which can either be constant or variable, depending on the optimization strategy.
The loss function converges towards zero for the optimal parameters $\theta$, in which case the gradient of the loss function vanishes. As a result, the parameters will hardly update anymore, indicating that we have reached an optimal solution.


\subsection{Solving eigenvalue problems}
\textcolor{red}{Nevland: Vektornotasjon på alle vektorer.}

Deep neural networks are capable of solving more or less any differential equation. We will in this section follow the work of Yi et. al. (2004) \cite{yi2004neural} closely, using the same first ODE as them and closely follow their methods for finding the eigenvalues. The first order ODE in question is given as     
\begin{equation}
	\frac{dx(t)}{dt} = -x(t) + f[x(t)],
	\label{eq:diff_eig}
\end{equation}
where
\begin{align}
	f(x) = [x^TxA + (1 - x^TAx)I]x
\end{align}

where $A$ is a real symmetric matrix of shape $(n\times n)$ and $t \ge 0$ represents time. The solution of the neural network is given by $x(t)$ with dimensions $(n_t\times n)$, where $n_t$ is the number of time steps. The ODE \eqref{eq:diff_eig} is applicable for finding the eigenvalues of $A$. An equilibrium point $\tilde{x}$ of a differential equation is a solution that is stationary, that is a state where the solution doesn't change anymore, mathematically expressed as $\frac{dx}{dt} = 0$. It can be shown that the equilibrium points of \eqref{eq:diff_eig} span the eigenspace of $A$, implying that the equilibrium points are precisely the eigenvectors of $A$. That is, any $\tilde{x}$ that satisfies
\begin{align}
	\frac{d\tilde{x}(t)}{dt} &= -\tilde{x} + f[\tilde{x}(t)] \nonumber \\
	&= -\tilde{x} + [\tilde{x}^T\tilde{x}A + (1 - \tilde{x}^TA\tilde{x})I]\tilde{x} \nonumber \\
	&= \tilde{x}^T\tilde{x}A\tilde{x} - \tilde{x}^TA\tilde{x}\tilde{x} = 0,
\end{align}
is an eigenvector of $A$. The rayleigh quotient formula can then be used to find the corresponding eigenvalue, given by
\begin{align} \label{eq:rayleigh_quotient}
	r = \frac{\tilde{x}^T\tilde{x}}{\tilde{x}^T A \tilde{x}}.
\end{align}

An essential question is how the neural network should be initialized. That is, what trial solution of the form \eqref{eq:NN_model} should be chosen? Convergence analysis reveal that any non-zero trial solution $x(0) \in \mathbb{R}^n$ will give a solution of \eqref{eq:diff_eig} that converges to an eigenvector of $A$. Moreover, if $x(0)$ is not orthogonal to the subspace spanned by the largest eigenvector $\lambda_{\mathrm{max}}$ of $A$, then the solution $x(t)$ converges to the corresponding eigenvector.

The differential equation \eqref{eq:diff_eig} is a first order nonlinear ODE. Generally, solving a nonlinear differential equation numerically is a comprehensive process involving discretization in time followed by linearization through an iterative method (e.g Newton's method). Explicit schemes are an exception though, as all nonlinear terms are of the previous time step, hence are known. This allow us to easily solve \eqref{eq:diff_eig} numerically using the forward euler method. Discretizing the equation and solving for next time step $n+1$ gives

\begin{align}
	\frac{x^{n+1}-x^n}{\Delta t} &= -x^n + f[x^n] \nonumber \\
	&= (x^n)^T x^n A x^n - (x^n)^T A x^n x^n, \\
	\Rightarrow x^{n+1} &= x^n + \Delta t\big[ (x^n)^T x^n A x^n - (x^n)^T A x^n x^n \big].
\end{align}

Starting with a non-zero initial condition $x^0 = I(x)$, convergence is fulfilled as long as the forward euler scheme is stable. The stability criteria depends on the discretization parameter $\Delta t$, and is to be approximated empirically. 



\section{Method}
\subsection{Unit testing}

Before the forward Euler scheme is applied to a particular problem, it is important to test that the discretized equations return expected results. Unit tests are constructed to test the implementation. This is done by manually calculating the solution of the first two time steps given the initial condition, and comparing the result with that obtained by the numerical scheme. Since the same recursive formula is used for all time steps, it is sufficient to test the two first time steps. To avoid too much computations we choose five equally sized intervals between $x=0$ and $x=L=1$, that is $\Delta x = 0.2$. The time step is set to $\Delta t = 0.01$, ensuring stability.
For time step $j$, that is $t=j\Delta t$, we have the following two boundary values and four interior points

\begin{align*}
	u_0^1 &= u_5^1 = 0 \\
	u_i^1 &= \frac{\Delta t}{\Delta x^2}u_{i+1}^0 + (1 - 2\frac{\Delta t}{\Delta x^2})u_i^0 + \frac{\Delta t}{\Delta x^2}u_{i-1}^0 \\
	&= 0.05u_{i+1}^0 + 0.9u_i^0 + 0.05u_{i-1}^0
\end{align*}

The initial condition used is $u_*^0 = \big(\sin(0),\:\sin(0.2\pi),\:\sin(0.4\pi),\:\sin(0.6\pi), \:\sin(0.8\pi),\:\sin(\pi)\big)$. If the results of the code match those manually calculated up to machine precision, it verifies the internal functionality of our implementation of forward euler. The results of the unit test are provided in Table \ref{tab:unit_test} in Appendix \ref{app:unit_test}.



\subsection{Neural network setup}

Our first step when designing our neural network is to decide on a architecture. We have studied this more in-depth in a previous report \cite{project2}. Because of this we will only briefly investigate how the depth (number of hidden layers) affects the loss \eqref{eq:NN_loss}. We use similar architecture as Raissi et. al. (2017) \cite{raissi2017physics}, namely 20 nodes in each hidden layer, with hyperbolic tangent as activation function in each hidden node. The depth we will test for is $N_h\in\{2,4,8\}$ hidden layers, then choose the architecture with minimal loss after 500 training iterations. We also plot how the loss evolves as a function of iterations to see the evolution of training. The network is set up using TensorFlow with linearly stacked dense layers (sequential) with standard Glorot normal initialization (also called Xavier normal initializer). As optimizer we use Adam, which is a stochastic gradient decent method based on adaptive estimation of first- and second-order moments. Because of previous success with dynamic learning rate in the preceding project \cite{project2}, we implement TensorFlow's piecewise constant decay schedule. Specifically we use learning rate $10^{-2}$ for the first 1000 steps, $10^{-3}$ from iterations 1000 to 3000 and $5 \cdot 10^{-4}$ for succeeding steps.

To train the network we need data, and we will use random data points uniformly distributed. In figure \ref{fig:NN_training_points} we display the data points used for training. Here we choose 50 data points for $t=0$ and $x\in[0,1]$, and another 50 points for $t\in[0,1]$ and $x\in\{0,1\}$. This is to make sure we also test for boundary points when calculating the loss \eqref{eq:NN_loss}, and will act as a regularization. Next we also choose $10^4$ data points $(t_i, x_i)$, where $t_i,x_i\in[0,1]$. 
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{../output/plots/training_points.pdf}
	\caption{In this plot we highlight the data points used for training the neural network. Points on the boundary are highlighted, and we use $50$ points for $t=0$, and a total of $50$ points at $x=0$ and $x=1$, resulting in a total of $100$ boundary points. We use $10^4$ data points not at the boundary.} \label{fig:NN_training_points}
\end{figure}

\textcolor{red}{Vikenes: Flyttes til sammenlikning}
After choosing the network with smallest loss after 500 iterations, we start to compare it with Forward Euler. We do this by comparing the MSE at two different timesteps $t\in\{0.1, 0.5\}$. We can then increase the number of training iterations and see how this affects the MSE, ideally it will be below that of Forward Euler. We also plot the difference between analytical $u_a$ solution and output $u_p$ of the network ($u_p - u_a$). This is so we can see how the output differ from the analytical solution.

\subsection{Forward Euler}

As discussed in the theory section, following equation \eqref{eq:stability}, $\alpha=0.5$ produce stable solutions for the diffusion equation. Throughout this report we will therefore use a time step of $\Delta t = \alpha \Delta x^2 = 0.5\cdot \Delta x^2$ for different choices of spatial resolution, and not consider other stability factors. We will limit our study to two spatial resolutions, namely $\Delta x = 0.1$ and $\Delta x = 0.01$.  

The first analysis we perform is the time evolution of the mean squared error (MSE) of the forward Euler solution. The MSE at a particular time step, $j$, is given as 
\begin{align} \label{eq:MSE}
	\mathrm{MSE_j} = \frac{1}{N}\sum_{i=0}^{N-1} (u_i^j - u_{e,i}^j)^2 
\end{align}
where $u_e$ denotes the analytical solution and $N$ is the number of spatial points. By construction the MSE should be zero at $t=0$. Subsequently, we expect an increasing MSE. Our boundary conditions results in the diffusion equation approaching $u(x,t)=0$ as $t$ increases, causing the MSE to eventually decline after a certain time. Combined with the unit test, this serves as a simple method of validating our result.  

\textcolor{red}{Vikenes: Flytt det under til sammenlikning}

To proceed, we will study the solutions at two different times, $t_1$ and $t_2$ for both $\Delta x = 0.1$ and $\Delta x = 0.01$. We choose $t_1=0.1$ and $t_2=0.5$, where $u(x,t_1)$ is significantly curved and $u(x,t_2)$ is almost linear. These times will serve as the foundation for comparing the performance of our explicit numerical solver to that of the neural network. With the forementioned time periods and spatial resolutions, we compare the MSE of the two models, using equation \eqref{eq:MSE}. 

\textcolor{red}{Vikenes: Bruk NN-notasjon for likning under. Må stemme med plot}
As a final study, we would like to plot the absolute difference between the numerical solutions and the analytical one for both $t_1$ and $t_2$. 
\begin{align} \label{eq:absolute_difference}
	\Delta u_i^n = u_i^n - u_{e,i}^n
\end{align}
Our main purpose of this is to see whether the neural network can circumvent one of the major drawbacks of the forward Euler scheme, giving an output that is either consistently above or below the actual solution. For more volitale differential equations, such errors can propagate throughout the simulation, resulting in a poor fit. One potential advantage of using neural networks will thus be if it yields unbiased errors. 


\subsection{Finding eigenvalues}
The matrix we will consider for our initial analysis is a real symmetric matrix, $A$, with dimensions $(3\cross3)$ \footnote{We didn't realize that it should have been a $(6\cross6)$ matrix until after the report was finished. This has been cleared with Morten.}. In the end we will perform one simulation with a $(6\cross6)$ matrix to study differences for higher dimensions. 

Using the neural network model \eqref{eq:diff_eig} to find the eigenvalues of $A$ requires a trial solution \eqref{eq:NN_model} to initiate the network. The trial solution must fulfill the convergence property to guarantee that we end up with the eigenvalues, as mentioned in the theory section. Hence, we define the initial condition $x_0 \in \mathbb{R}^n$ \textcolor{red}{Nevland: Vektordimensjon} to be a vector of randomly generated numbers between zero and one. Since we don't know the eigenspace corresponding to the largest eigenvalue a priori, we have no guarantee that the trial solution is non-orthogonal to the eigenspace corresponding to the largest eigenvalue. To increase the probability of non-orthogonality, we can add random perturbations.

Even though a non-zero trial solution is the only requirement for convergence, there are strategies for improving the \textit{rate} of convergence. If we have some preknowledge of a solution of the differential equation, incorporating this in the trial solution helps accelerate the learning of the neural network. The ODE \eqref{eq:diff_eig} is of first order with a term involving $dx/dt$ on one side and a term involving $x$ on the other. This is reminiscent of an exponential solution. Therefore, a smart initialization strategy of \eqref{eq:NN_model} is 

\begin{align}
	g_{\theta}(t) &= g_0(x(t)) + f[x(t),N(x, \theta)] \nonumber \\
	&= e^{-t}x_0 + (1 - e^{-t})N(x(t), \theta).
\end{align}

This expression fulfills the condition of a trial solution, namely that it must return the initial condition $x_0$ for time $t=0$. Once we have a trial solution we feed it to the neural network, which will calculate the gradient, $dx/dt$, and the resulting residual of $\eqref{eq:diff_eig}$. The MSE of the residual act as the loss function that is to be optimized through gradient descent. The accuracy of the final output of the neural network is assessed by comparing with the result of numpy's own eigenvalue solver. 

For comparison purposes, the forward euler scheme is initiated with the same initial condition $x_0$ as for the neural network model, and using the same temporal domain. Unless specified, a total time of $T=5$ is used, with a learning rate and number of epochs set to $0.005$ and $2000$ respectively, for the neural network.

For each simulation, we find the eigenvector, $\mathbf{v}_\mathrm{max}=(v_1,\,v_2,\,v_3)$, with the largest corresponding eigenvalue, $\lambda_\mathrm{max}$. We then plot the computed value of the three eigenvector components as a function of time, for both Forward Euler and Neural network. For a given simulation we also plot the time evolution of the Rayleigh quotient \eqref{eq:rayleigh_quotient}, for the two forementioned methods.   


\section{Results}

\subsection{Finding a suitable Neural network}

Table \ref{tab:NN_architecture_loss} shows the loss of the neural network after $500$ iterations for the three different network architectures. We see that $4$ hidden layers clearly yields the lowest value for the final loss. \textcolor{red}{Vikenes: tabelltekst}
\begin{table}[h]
	\centering
	\begin{tabular}{|c|l|}
		\hline
		Architecture & Loss   \\ \hline
		$8\cross20$         & 0.0048 \\ \hline
		$4\cross20$         & 0.00079 \\ \hline
		$2\cross20$         & 0.0022 \\ \hline
	\end{tabular}
	\caption{tabll}
	\label{tab:NN_architecture_loss}
\end{table}

The loss of the network as a function of iterations is shown in figure \ref{fig:Error_NN_architecture} for three different architectures, using $2$, $4$ and $8$ hidden layers for the upper left, upper right and lower left panel, respectively. 

\begin{figure}[h]
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{../output/plots/NN_diffusion_error_Nn20_Nh2.pdf}
	\endminipage\hfill
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{../output/plots/NN_diffusion_error_Nn20_Nh4.pdf}
	\endminipage\hfill
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{../output/plots/NN_diffusion_error_Nn20_Nh8.pdf}
	\endminipage\hfill
	\minipage{0.49\textwidth}
	\caption{Cost function of the neural network as a function of iterations, using $20$ hidden nodes in each network. The number of hidden layers in the network are $2,\,4$ and $8$, for the upper left, upper right and lower left panel, respectively.} \label{fig:Error_NN_architecture}
	\endminipage
\end{figure}

We notice large fluctuations of the loss for the network with $4$ hidden layers. Using logarithmic scales on the y-axes in figure \ref{fig:Error_NN_architecture}, the amplitudes of the oscillating losses appear bigger than they are in reality. As planned, we choose the network with $N_h=4$ for further analysis, as it yields the lowest MSE after $500$ iterations. A detailed study of different architectures is outside the scope of this report, and is nonetheless dependent of the particular PDE in question. 


Figure \ref{fig:NN_architecture_solution} shows the output of the neural network. We clearly see the exponential decay of the initial sine function, as expected, eventually flattening out, as $t\to1$.   
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{../output/plots/NN_diffusion_solution_Nn20_Nh4.pdf}
	\caption{The solution of the diffusion equation of the final neural network.} \label{fig:NN_architecture_solution}
\end{figure}


\subsection{Error of Forward Euler scheme}

To assess the accuracy of our implemented numerical scheme, we need to test the how well the produced solution fits the analytical solution. Figure \ref{fig:FE_MSE} shows the MSE as function of time for the forward euler scheme (note the scaling of the y-axis). Keeping in mind that we choose the time step to give a stability factor of $\alpha=0.5$. 

The MSE in figure \ref{fig:FE_MSE} increases significantly for early time steps, reaching a peak at around $t_1=0.1$ and quickly reduces thereafter. At $t_2=0.5$ the MSE is significantly lower compared to $t_1$.  

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../output/plots/MSE_FE_dx_001.pdf}
	\caption{Mean squared error of approximated solution by forward euler, using $\Delta x=0.01$ and time step $\Delta t$ dictated by the stability criterion \eqref{eq:stability}.}
	\label{fig:FE_MSE}
\end{figure}


\subsection{Comparison of error}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{../output/plots/error_FE_x_dx_001.pdf}
	\caption{The difference between the analytical solution of the diffusion equation and the solution obtained by forward euler at two different points in time.}
	\label{fig:FE_absolute_difference}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{../output/plots/final_compare_NN.png}
	\caption{In this plot ...} \label{fig:NN_relative_error}
\end{figure}

Figure \ref{fig:FE_absolute_difference} shows the plot of equation \eqref{eq:absolute_difference}, i.e. the numerical solution of the diffusion equation from forward Euler, subtracted by the analytical solution in equation \eqref{eq:analytical_solution_diffusion}. The corresponding plot for the neural network is shown in figure \ref{fig:NN_relative_error}. Using $\Delta x=0.1$ for forward Euler yielded an identically shaped plot as figure \ref{fig:FE_absolute_difference}, but with increased values on the $y$-axis, and we have therefore chosen to omit it. 

\textcolor{red}{Vikenes: Kommenter plott litt}

Table \ref{tab:NN_MSE_iterations} shows the resulting MSE of the neural network at $t_1$ and $t_2$ after four different number of training iterations. Except from an increase for $t_1$ when we go from $500$ to $1000$ training iterations, the MSE decreases as we increase the number of iterations. The increase is likely due to the fluctuations we saw in figure \ref{fig:Error_NN_architecture}.

\textcolor{red}{Sigurd: Tabelltekst}

\begin{table}[h]
	\centering
	\begin{tabular}{l|ll|}
		\cline{2-3}
		& \multicolumn{2}{l|}{Mean Square error} \\ \hline
		\multicolumn{1}{|l|}{Number of training iterations} & \multicolumn{1}{l|}{$t_1 = 0.1$} & $t_2 = 0.5$ \\ \hline
		\multicolumn{1}{|l|}{500}                           & \multicolumn{1}{l|}{0.0017}  & 0.0070  \\ \hline
		\multicolumn{1}{|l|}{1000}                          & \multicolumn{1}{l|}{0.015}   & 0.00055 \\ \hline
		\multicolumn{1}{|l|}{$10^4$}                          & \multicolumn{1}{l|}{$1.4\cdot 10^{-4}$}  & $9.5\cdot10^{-5}$  \\ \hline
		\multicolumn{1}{|l|}{$5\cdot10^4$}                        & \multicolumn{1}{l|}{$1.0\cdot10^{-6}$}        & $3.0\cdot10^{-6}$        \\ \hline
	\end{tabular}
	\caption{MSE NN }
	\label{tab:NN_MSE_iterations}
\end{table}

Table \ref{tab:MSE_compare} compares the MSE obtained for forward euler and neural network at two selected time steps $t$. With forward Euler we have included both spatial steps $\Delta x$, while we only consider $\Delta x=0.01$ for the neural network. 

\begin{table}[h]
	\centering
	\begin{tabular}{l|cc|cc|}
		\cline{2-5}
		& \multicolumn{2}{c|}{\textbf{$\Delta x=0.1$}}           & \multicolumn{2}{c|}{\textbf{$\Delta x=0.01$}}           \\ \cline{2-5} 
		& \multicolumn{1}{l|}{\textbf{FE}}         & \textbf{NN} & \multicolumn{1}{l|}{\textbf{FE}}          & \textbf{NN} \\ \hline
		\multicolumn{1}{|l|}{\textbf{$t_1=0.1$}} & \multicolumn{1}{l|}{$1.72\cdot 10^{-5}$} & -            & \multicolumn{1}{l|}{$1.81\cdot 10^{-9}$}  & $1.0\cdot10^{-6}$            \\ \hline
		\multicolumn{1}{|l|}{\textbf{$t_2=0.5$}} & \multicolumn{1}{l|}{$1.50\cdot 10^{-7}$} & -            & \multicolumn{1}{l|}{$1.69\cdot 10^{-11}$} & $3.0\cdot10^{-6}$             \\ \hline
	\end{tabular}
\caption{MSE as function of spatial step $\Delta x$ for two different time levels, for forward euler scheme and neural network. Forward euler is abbreviated as FE and neural network as NN.}
\label{tab:MSE_compare}
\end{table}

\subsection{Eigenvalue problem}


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{../output/plots/eigvec_T5_N1000.pdf}
	\caption{Predictions by forward euler (FE) and neural network (NN) of the eigenvector corresponding to the largest eigenvalue of a real, symmetric $3\times 3$ matrix. The horizontal dotted lines are the components of the eigenvector as found by NumPy.}
	\label{fig:eigvec_T5_N1000}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{../output/plots/eigval_T5_N1000.pdf}
	\caption{Rayleigh quotients of forward euler (FE) and neural network (NN) representing predictions of the largest eigenvalue of a real, symmetric $3\times 3$ matrix. The horizontal dotted line is the largest eigenvalue as found by NumPy.}
	\label{fig:eigval_T5_N1000}
\end{figure}

Figure \ref{fig:eigvec_T5_N1000} shows predictions of the eigenvector corresponding to the largest eigenvalue of a symmetric $(3\times 3)$ matrix $A$ of randomly assigned values between zero and one. The simulation is run for a total time of $T=5$ with $N=1000$ estimations in time, providing a time step of $\Delta t = 0.005$. The prediction from both forward euler and neural network converges to the true eigenvector, as obtained from NumPy. The largest eigenvalue is shown in Figure \ref{fig:eigval_T5_N1000} together with the rayleigh quotients estimated by forward euler and the neural network.

\textcolor{red}{Nevland: Skriv at figurene fremover har små akser, men at de er delvis uendra. Skriv hvorfor vi gjør det.}

\textcolor{red}{Nevland: Fjern $\Delta t=0.05$. Husk diskusjon}
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigvec_T5_N100.pdf}
		\caption{Eigenvectors}
		\label{fig:eigvec_T5_N100}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigval_T5_N100.pdf}
		\caption{Rayleigh quotients}
		\label{fig:eigval_T5_N100}
	\end{subfigure}
	\caption{Predictions by forward euler (FE) and neural network (NN) of largest eigenvalue and corresponding eigenvector of a real, symmetric $3\times 3$ matrix, using a time step $\Delta t = 0.05$. The $x$ and $y$ axes are identical to those in figure \ref{fig:eigvec_T5_N1000} and \ref{fig:eigval_T5_N1000}}
	\label{fig:eig_T5_N100}
\end{figure}


\begin{figure}[h]
	
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigvec_T5_N10.pdf}
		\caption{Eigenvectors}
		\label{fig:eigvec_T5_N10}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigval_T5_N10.pdf}
		\caption{Rayleigh quotients}
		\label{fig:eigval_T5_N10}
	\end{subfigure}
	\caption{Predictions by forward euler (FE) and neural network (NN) of largest eigenvalue and corresponding eigenvector of a real, symmetric $3\times 3$ matrix, using a time step $\Delta t = 0.5$. The $x$ and $y$ axes are identical to those in figure \ref{fig:eigvec_T5_N1000} and \ref{fig:eigval_T5_N1000}}
	\label{fig:eig_T5_N10}
\end{figure}



Figure \ref{fig:eigvec_T5_N100} and \ref{fig:eigvec_T5_N10} shows predictions of the same eigenvector of the same matrix $A$ in Figure \ref{fig:eigvec_T5_N1000} but with a time step of $\Delta t = 0.05$ and $\Delta t = 0.5$, respectively. The largest eigenvalue and rayleigh quotients are shown in Figure \ref{fig:eigval_T5_N100} and \ref{fig:eigval_T5_N10}.
The graphs are less smooth due to larger timesteps and fewer estimates. 

\begin{figure}[h]
	
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigvec_T5_N1000_eta01.pdf}
		\caption{Eigenvectors}
		\label{fig:eigvec_T5_N1000_eta01}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigval_T5_N1000_eta01.pdf}
		\caption{Rayleigh quotients}
		\label{fig:eigval_T5_N1000_eta01}
	\end{subfigure}
	\caption{Predictions of largest eigenvalue and corresponding eigenvector by forward euler and neural network for a real, symmetric $3\times 3$ matrix, using a learning rate of $0.1$ and a time step $\Delta t = 0.005$. The $x$ and $y$ axes are identical to those in figure \ref{fig:eigvec_T5_N1000} and \ref{fig:eigval_T5_N1000}}
	\label{fig:eig_T5_N1000_eta01}
\end{figure}

\begin{figure}[h]
	
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigvec_T5_N1000_epochs50.pdf}
		\caption{Eigenvectors}
		\label{fig:eigvec_T5_N1000_epochs50}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigval_T5_N1000_epochs50.pdf}
		\caption{Rayleigh quotients}
		\label{fig:eigval_T5_N1000_epochs50}
	\end{subfigure}
	\caption{Predictions of largest eigenvalue and corresponding eigenvector by forward euler and neural network for a real, symmetric $3\times 3$ matrix, using a learning rate of $0.1$, $50$ epochs and a time step $\Delta t = 0.005$. Other than negative eigenvector components in the left panel, and lower Rayleigh quotient in the right panel, the $x$ and $y$ axes are identical to those in figure \ref{fig:eigvec_T5_N1000} and \ref{fig:eigval_T5_N1000}}
	\label{fig:eig_T5_N1000_epochs50}
\end{figure}


Adjusting hyperparameters of the neural network has also been investigated. Figure \ref{fig:eigvec_T5_N1000_eta01} shows the predicted eigenvector using a time step of $\Delta t = 0.005$ and a learning rate of $0.1$. The associated rayleigh quotients is shown in Figure \ref{fig:eigval_T5_N1000_eta01}. Predictions of the same eigenvector is shown in Figure \ref{fig:eigvec_T5_N1000_epochs50}, with the learning rate retained at $0.1$ but the number of epochs reduced from $2000$ to $50$. The associated rayleigh quotients are shown in Figure \ref{fig:eigval_T5_N1000_epochs50}.


\begin{figure}[h]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigvec_T5_N6.pdf}
		\caption{Eigenvectors}
		\label{fig:eigvec_T5_N6}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/eigval_T5_N6.pdf}
		\caption{Rayleigh quotients}
		\label{fig:eigval_T5_N6}
	\end{subfigure}
	\caption{Predictions of largest eigenvalue and corresponding eigenvector by forward euler and neural network for a real, symmetric $3\times 3$ matrix. A time step of $\Delta t = 0.83$ is used. Other than increased eigenvector component values for forward Euler on the left panel, the $x$ and $y$ axes are identical to those in figure \ref{fig:eigvec_T5_N1000} and \ref{fig:eigval_T5_N1000}}
	\label{fig:eig_T5_N6}
\end{figure}

\textcolor{red}{Nevland: Flytt figur 13 oppover} Results of a further increase in the time step is shown in Figure \ref{fig:eig_T5_N6}, using $\Delta t= 0.83$ corresponding to $N=6$ estimations, where the left panel shows the eigenvector components and the right panel shows the associated Rayleigh quotients. 

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/FE_eigvec_T2_N1000_dim6.pdf}
		\caption{Forward Euler}
		\label{fig:FE_eigvec_T2_N1000_dim6}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../output/plots/NN_eigvec_T2_N1000_dim6.pdf}
		\caption{Neural network}
		\label{fig:NN_eigvec_T2_N1000_dim6}
	\end{subfigure}
	\caption{Predictions of eigenvector corresponding to the largest eigenvalue for a real, symmetric $6\times 6$ matrix. A time step of $\Delta t = 0.005$ and a total time $T=2$ is used.}
	\label{fig:eigvec_T2_N1000_dim6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{../output/plots/eigval_T2_N1000_dim6.pdf}
	\caption{Rayleigh quotient corresponding to the largest eigenvalue of a real, symmetric $6\times 6$ matrix.}
	\label{fig:eigval_T2_N1000_dim6}
\end{figure}

\textcolor{red}{Nevland: Stack 6x6 figurer} An additional simulation using a $6\times 6$ matrix has also been conducted. Figure \ref{fig:eigvec_T2_N1000_dim6} shows the predicted eigenvector for forward euler (\ref{fig:FE_eigvec_T2_N1000_dim6}) and neural network (\ref{fig:NN_eigvec_T2_N1000_dim6}), and Figure \ref{fig:eigval_T2_N1000_dim6} the rayleigh coefficient corresponding to the largest eigenvalue of a $6\times 6$ symmetric matrix. A time step of $\Delta t = 0.005$ have been used and a total time of $T=2$. \textcolor{red}{Nevland: Skriv eksplisitt begrunnelse for T=2} The reason for the latter choice is based on the previous simulations with $\Delta t=0.005$, as both forward euler and neural network converge long before $T=5$. Reducing the simulation time makes it easier to illustrate the convergence property of the six eigenvector components.
 
 
\clearpage
\section{Discussion}

\subsection{Solving the diffusion equation with Forward Euler}
Figure \ref{fig:FE_MSE} verifies that simulating the forward euler scheme of the diffusion equation with spatial step $\Delta x = 0.01$ yields an excellent correspondance between the numerical and analytical solution. The maximum error is obtained at an early time step, around $\Delta t = 0.1$. For time levels beyond this, the MSE decreases exponentially and eventually converges towards zero. Notice that the error is exactly zero initially, because the numerical scheme has enforced the solution at the initial time step to satisfy the initial condition from the analytical solution. The maximum error is on the order of $10^{-9}$, yielding an excellent fit to the exact solution. One would not be able to distinguish the numerical solution from the exact solution if they were plotted together.

\par The shape of the plot is in good correspondance with what was initially expected, as elaborated on in Methods. The large initial increase of the MSE is a result of the curved initial solution. The spatial gradients are significant and the approximation with finite differences becomes less accurate.
In the beginning of the diffusion process, for low $t$, the solution changes rather quickly. The forward euler scheme is first order accurate in time, $O(\Delta t)$, and second order accurate in space, $O(\Delta x^2)$, implying that the numerical solution is more sensitive to the temporal evolution than the spatial evolution. Hence, the forward euler scheme will produce larger errors for the initial time levels due to the rapid change in solution \cite{Linge2017}.
\par After time $t=0.1$ the MSE decreases. This can be explained by the discretization parameters $\Delta t$ and $\Delta x$. The magnitude of the gradients are diminished because the solution becomes more linear and less curved. As a result, the solution changes more slowly, allowing the finite difference scheme to approximate the changes more accurately both spatially and temporally. Eventually, the exact solution is approximately constant at zero. In this case, the gradients of the solution are indiscernible and the numerical scheme is able to approximate the solution with machine precision. Figure \ref{fig:FE_MSE} illustrates this by the MSE converging towards zero sufficiently deep into the diffusion process.

\subsection{Solving the diffusion equation with a neural network}

\par \textcolor{red}{Et forsøk paa aa forklare fluktuasjonene i MSE for NN:} The reason that the error flucuates instead of decreasing monotonically is that we use the Adam optimizer \cite{kingma2017adam}. This is a type of stochastic optimization method, providing more favorable features than traditional stochastic methods. The stochasticity allows the neural network to explore more of the convex parameter space by trying out new, potentially worse solutions. The hope is that, after escaping the local optima, that we find an even better solution at some later stage. The apparent jump in Figure \ref{fig:error_NN_10000} is an example of escaping a local optima at the expense of further exploration. In this case, it does not seem like the new solutions lead to better performance than the previous optima. It eventually converges to zero, which is something the solutions prior to the jump did as well. 

\subsection*{Using Forward Euler and Neural Network to find eigenvalues}
Forward euler and neural networks are two different numerical methods capable of finding the eigenvalues of a real, symmetric matrix $A$ by solving the ODE in equation \eqref{eq:diff_eig}. The two algorithms for solving differential equations are quite different, and will therefore yield different predictions of the largest eigenvalue. The result of forward euler is sensitive to the time step $\Delta t$. Thus, it is of interest to compare the accuracy of prediction of the two methods when the time step is altered. 

% FE: N=1000, N=100, N=10
Comparing Figure \ref{fig:eigvec_T5_N1000}, Figure \ref{fig:eigvec_T5_N100} and Figure \ref{fig:eigvec_T5_N10} for time steps $\Delta t = 0.005$, $\Delta t = 0.05$ and $\Delta t = 0.5$, respectively, we observe that the accuracy of forward euler consistently decreases for larger time steps. The result for $\Delta t=0.5$ is particularly biased with all components of the predicted eigenvector consistently missing their respective targets. The corresponding rayleigh quotients, shown in Figure \ref{fig:eigval_T5_N1000}, Figure \ref{fig:eigval_T5_N100} and Figure \ref{fig:eigval_T5_N10}, converge precisely to the largest eigenvalue calculated by \code{numpy.linalg.eig}. Thus, despite the lower accuracy - particularly for forward euler - both methods succeed in returning the largest eigenvalue with four decimal precision.

It may appear surprising that the significant bias of the estimated eigenvector for forward euler for $\Delta t =0.5$ is not replicated in the associated rayleigh quotient, which converges precisely to the largest eigenvalue. To explain this, observe that the \textit{relation} between the components of the estimated eigenvector seems to replicate that of the true eigenvector, only shifted towards higher values. If $v$ is the eigenvector of matrix $A$, the associated eigenvalue is given by
\[ \lambda = \frac{v^T A v}{v^T v} \]
Then, if $s$ a constant representing the shift, the rayleigh quotient becomes

\begin{align*}
	r &= \frac{(sv)^T A (sv)}{(sv)^T (sv)} = \frac{s^2 v^T A v}{s^2 v^T v} \\
	&= \frac{v^T A v}{v^T v}
\end{align*}

This implies that the associated eigenvalue is unaffected by scaling the components of an eigenvector by a constant amount. Geometrically, it means that the eigenvector is extended or shrunk, but the spanned eigenspace is the same, hence the same eigenvalue. \textcolor{red}{Gir dette mening, Vikenes, Sigurd? Vikenes: Ja.} Figure \ref{fig:eig_T5_N1000_eta01} is an exception, though. For this particular simulation, the relation between the first and third predicted eigenvectors are different from the relation of the true eigenvectors, whereas the rayleigh quotient converges to the largest eigenvalue. This does not comply with the argument above, and we find it difficult to interpret this particular result. It should incentivate for further analysis.

% NN: N=1000, N=100, N=10
The neural network on the other hand, apparently looks unaffected by the increase in time step. In fact, it seems like the neural network has a more accurate convergence for $\Delta t = 0.5$ than $\Delta t = 0.05$ and $\Delta t = 0.005$. Still, this does not imply that the accuracy increases with larger time step in general, but it substantiates the fact that the convergence property of a neural network is more sensitive to other features than the time step. 

% NN: Dependence on other params than time step
In project 2 we studied some of the most sensitive parameters for a neural network to provide accurate predictions, including the learning rate, number of epochs and loss function. The dependence on the learning rate is illustrated by comparing Figure \ref{fig:eigvec_T5_N1000} with \ref{fig:eigvec_T5_N1000_eta01}. Increasing the learning rate from 0.005 to 0.1 has consequences for the result. The neural network now predicts an eigenvector whose components notably deviate from that of the true eigenvector. From Figure \ref{fig:eigval_T5_N1000_eta01} the rayleigh quotients corresponds quite well with the largest eigenvalue, just slightly less accurate. Additionally, if the number of epochs is reduced to 50, the consequences are even more severe, as shown in Figure \ref{fig:eigvec_T5_N1000_epochs50}. Now the predicted components completely misses the true components of the eigenvector. Figure \ref{fig:eigval_T5_N1000_epochs50} shows that the rayleigh quotients of the neural network apparently converges to a different value than the largest eigenvalue. Because forward euler only depends on $\Delta t$, it is unaffected by a change in the learning rate and number of epochs. Conclusively, the results indicate that a learning rate of $0.1$ and $50$ number of epochs is not a good architecture of a neural network for solving equation \eqref{eq:diff_eig}.


% FE & NN: N=1000, N=100, N=10
Figure \ref{fig:eigvec_T5_N6} shows that the eigenvector components do not converge but rather oscillate in an inconsistent pattern when we use Forward Euler with a time step $\Delta t =0.83$. There is a strong indication of diverging values for the eigenvector components, which would be more apparent if we either increased the duration of the simulation, or decreased the time step even further. 

The corresponding rayleigh quotients, shown in Figure \ref{fig:eigval_T5_N6}, verify that forward euler is not able to converge to the true solution of equation \eqref{eq:diff_eig} as $\Delta t$ is too large. All simulations with $\Delta t \leq 0.5$ have given a relative error less than $1\%$ between the final rayleigh quotient and the largest eigenvalue using forward euler. In comparison, the relative error for $\Delta t = 0.83$ is $22.1\%$.
 
All simulations so far have been conducted for a symmetric $3\times 3$ matrix. It is of interest to compare the two methods for a higher dimensional matrix as well. This is an important step towards generalization of the ability of forward euler and neural network to find eigenvalues of symmetric matrices of arbitrary shape. Figure \ref{fig:eigvec_T2_N1000_dim6} and Figure \ref{fig:eigval_T2_N1000_dim6} simulates for a $6\times 6$ symmetric matrix. The plot shows that neither forward euler nor neural network have problems with convergence. Apparently, the predicted components of the eigenvector are near perfect for both methods after time $T=2$. The stochastic initialization of the neural network will yield different results for different simulations, some that may give notable deviations. Nonetheless, the rayleigh quotient converges to the largest eigenvalue with forth decimal precision for both neural network and forward euler. The only noticable difference is that forward euler has a smoother convergence, while the neural network seems to possess some minor low-frequency fluctuations. The simulation illustrates that forward euler is slightly more consistent for small time steps $\Delta t$, even for higher dimensional matrices.

Taking all simulations into account, the neural network has excellent accuracy for $\Delta t < 1$. This indicates weak dependency on the time step, but this is completely deafed by the sensitivity to other hyperparameters such as learning rate and number of epochs.



\subsection{Potential for solving general differential equations}
The forward euler method and neural networks are two quite different methods for solving differential equations. The former relies on discretization of the derivative to arrive at an explicit recursive set of equations to solve for each time step. The latter uses the residual error from the approximation as a basis for backpropagation to update weights and biases to improve the approximation to the actual solution. Figure \ref{fig:error_NN_40000} visualizes how bad the approximated solution of the initial run of the neural network is. Just after a few iterations though, the error decreases tremendously and goes below 0.083 (which is the total error obtained from forward euler) after around 100 iterations. After this, the total error fluctuates mildly but eventually converges towards zero.
 
\par The forward euler method clearly has an advantage in its simple recursive formula and fast computation time. The proposed solution is calculated within a few milliseconds, with a total error of only 0.083. On the other hand, it is limited by a stability requirement in order to produce realistic results. This effectively puts a restriction on applicable mesh resolutions.

\par A notable drawback of the neural network is the demanding training process. To compute an approximated solution of \eqref{eq:diffusion_equation_1D} requires calculating gradients and accumulating chained derivatives backwards through the hidden layers. This is a significantly larger computational effort than calculating the simple recursive formula of the forward euler scheme. As a result, the neural network suffers from a large computational runtime. Still, if training sufficiently long, the neural network will produce a better solution than forward euler and eventually yield a near perfect approximation to the exact solution. 

\par The forward euler scheme has proven to obtain good accuracy on the numerical solution. However, forward euler has an inherent disadvantage in that it is conditionally unstable with stability criteria given by \eqref{eq:stability}. In general it means that the forward euler scheme is not a stable method for solving differential equations as it requires that we carefully assign the mesh discretization steps. Neural networks are not limited to any kind of discretization. Neural networks benefit from good convergence properties. That is, given enough time to train it is able to approximate the actual solution of a differential equation with an error of approximately zero. Moreover, the neural network model \eqref{eq:NN_model} has the additional flexibility in that the function $f$ can be tweaked to give a better initial guess of the solution. 
\par Overall, the method of choice for solving differential equations is a tradeoff between the accuracy of the approximated solution and the computational runtime. Forward euler is definitely the desired method if we want a quick representation of the solution. If the importance is how precise the representation is, a neural network is a better choice.

\textcolor{red}{Det meste av diskusjonen over kan beholdes da det gjelder generelt, men de faktiske tallene nevnt maa oppdateres med resultatene fra MSE.}


\subsection{Eigenvalue problems}
The potential of solving differential equations spans a large domain of mathematical problems. Particularly, the model \eqref{eq:diff_eig} can be represented by a neural network or a forward euler scheme to approximate the eigenvalues of a real, symmetric matrix. Our experiments have shown that the neural network has no trouble finding an accurate approximation to the largest eigenvalue and the corresponding eigenvector given a suitable trial solution and network architecture. The parameter space of a neural network is large, and optimizing a solution requires an infeasible search over all parameters. Training for more epochs gives a more accurate prediction, but comes at the expense of a longer runtime, in which case forward euler can produce equally accurate results for a much shorter runtime. Nonetheless, the optionality to tune the various parameters of a neural network makes it a flexible method for finding approximate solutions.

The forward euler scheme gives a more accurate prediction then the neural network, in addition to converging faster, as long as the time step is sufficiently low. In this case, forward euler is the prevailing method for finding the largest eigenvalue of a real, symmetric matrix. However, the method has an inherent disadvantage in terms of stability. Our experiments indicate that forward euler diverges for $\Delta t \ge 1$. This marks a transition where the results of forward euler are deceptive and the neural network is the method of choice.

Most of the differential equations frequently applied in natural science, such as the diffusion equation, are simple enough for finite difference schemes to handle efficiently and accurately. However, for more intricate differential equations finite differencing may not give accurate approximations of gradients. Refining the mesh is a solution, but this is computationally expensive. For situations where we want to simulate for a very long period refining the mesh may not be a feasible option. For these particular cases, it seems like neural network is the prominent method and a great alternative to conventional numerical schemes. Hence, the method of choice is problem specific. 

\textcolor{red}{Vikenes: Burde kommentere at NN vil være bedre/raskere for mer komplekse modeller, og at NN er best dersom man skal simulere over lange perioder.}



\section{Conclusion}
In this project we have particularly studied the diffusion equation, a rather simple PDE. It has a closed form expression for the analytical solution, so strictly speaking it is not necessary to solve the equation numerically. Still, it is a good representation for comparing numerical methods. Our results have proven that forward euler clearly outperforms neural network in terms of MSE between approximated and analytical solution, as long as it fulfills the stability requirement \eqref{eq:stability}. The potential for finding eigenvalues by solving the first order ODE \eqref{eq:diff_eig} has also been studied, in which case forward euler and neural network perform comparably well, although forward euler generally runs faster. 

The two differential equations studied in this project are linear and rather simple differential equations. In a general case, differential equations can be much more intrinsic. In such cases, numerical schemes and particularly forward euler encounter problems with accurate finite differences. Neural networks, on the other hand, can according to the universal approximation theorem approximate any function to arbitrary accuracy without any stability constraint on the time step. Hence, neural network is a more flexible numerical technique with a wide applicablility.

\section{Appendix}

\subsection{Analytical solution of the diffusion equation} \label{app:1D_diff_eq}
By using the concept of separation of variables, the solution can be expressed as
\[ u(x,t) = X(x)T(t) \]
The solution is separated into a function $X$ only depending on the independent variable $x$, and a function $T$ only depending on the independent variable $t$. Equation \eqref{eq:diffusion_equation_1D} can then be rewritten as

\begin{align*}
	\frac{\partial^2 X(x)T(t)}{\partial x^2} &= \frac{\partial X(x)T(t)}{\partial t} \\
	T(t)\frac{\partial^2 X(x)}{\partial x^2} &= X(x)\frac{\partial T(t)}{\partial t} \\
	\frac{1}{X}\frac{\partial^2 X(x)}{\partial x^2} &= \frac{1}{T}\frac{\partial T(t)}{\partial t}
\end{align*}

The core of the method now becomes clear. The independent variables are separated and put on either side of the equation. Because $x$ and $t$ are independent we may fix one of them, say $x$, while letting the other ($t$) vary. The left side of the equation is thus constant, and since we have equality the expression on the right side must equal the same constant, for arbitrary $t$. Therefore, we can set the left side and right side equal to a constant $-k^2$. The reason for defining a negative constant is to prevent a growing solution, which will be clear on the derivation.

\[ \frac{1}{X}\frac{\partial^2 X(x)}{\partial x^2} = \frac{1}{T}\frac{\partial T(t)}{\partial t} = -k^2\]

This is solved for the functions $X$ and $T$ separately. 

\begin{align*}
	\frac{1}{T} \frac{dT}{dt} &= -k^2 \\
	\int \frac{1}{T}\:dT &= -\int k^2\:dt \\
	\ln T &= -k^2 t + \hat{T_0} \\
	T &= e^{-k^2t + \hat{T_0}} \\
	&= T_0 e^{-k^2t}
\end{align*}

\begin{align}
	\frac{1}{X} \frac{\partial^2 X}{\partial x^2} &= -k^2 \\
	\frac{d^2 X}{dx^2} &= -k^2 X \\
	X &= X_0 \sin(kx) + X_1 \cos(kx)
\end{align}

Here we have used the fact that a differential equation of the form $X'' = -X$ has a trigonometric solution. The general solution becomes

\[ u(x,t) = X(x)T(t) = T_0 e^{-k^2 t} \big[X_0\sin(kx) + X_1 \cos(kx)\big] \]

To obtain a specific solution we use the initial and boundary conditions to find the undetermined coefficients $T_0$, $X_0$ and $X_1$. At $x=0$ we have

\begin{align*}
	u(0,t) &= 0 \\
	T_0 e^{-k^2 t} X_1 &= 0 
\end{align*}

The only possibility is $X_1 = 0$ because if $T_0=0$ then the general solution becomes time-independent, and does not describe the temporal variability of  \eqref{eq:diffusion_equation_1D}. At $x=L$ we have

\begin{align*}
	u(L,t) &= 0 \\
	T_0 e^{-k^2 t} X_0 \sin(kL) &= 0
\end{align*}

Following the previous argument we can't have $X_0=0$ as this would imply independence of space. Instead, we must have 

\begin{align*}
	\sin(kL) &= 0 \\
	kL &= n\pi \\
	k &= \frac{n\pi}{L}, \quad n=0,1,2,\dots
\end{align*}

Defining a new constant $A=T_0 X_0$ we get the following solution for an eigenfrequency $n$.

\[ u(x,t) = A e^{-k^2t} \sin(\frac{n\pi x}{L}) \]

Because any linear combination of a solution is a also a solution, the full specific solution is given by the linear combination of all eigenfrequencies $n$.

\[ u_n(x,t) = \sum_{n=1}^{\infty} A_n e^{-k_n^2 t} \sin(\frac{n\pi x}{L}) \]

The coefficients $A_n$ are determined by the initial condition.

\begin{align*}
	u_n(x,0) &= \sin(\pi x) \\
	\sum_{n=1}^{\infty} A_n \sin(\frac{n\pi x}{L}) &= \sin(\pi x) 
\end{align*}

This is a Fourier sine series with the following expression for the coefficient.

\begin{align*}
	A_n &= \frac{2}{L} \int_0^L \sin(\pi x)\sin(\frac{n\pi x}{L})\:dx \\
	&= 2 \int_0^1 \sin(\pi x)\sin(n\pi x)\:dx
\end{align*}

Note that for $n \ne 1$, the two factors in the integrand are mutually orthogonal functions on domain $x \in [0,1]$. Therefore, the integral vanishes. For $n=1$ we have

\begin{align*}
	A_1 &= 2 \int_0^1 \sin^2(\pi x)\:dx \\
	&= 2\cdot \frac{1}{2} \int_0^1 [1 - \cos(2\pi x)]\:dx \\
	&= [x - \frac{1}{2\pi}\sin(2\pi x)]_0^1 \\
	&= 1
\end{align*}

This gives
\[ A_n = \begin{cases}
	1, & n=1 \\
	0, & n\ne 1
\end{cases} \]

In other words, the specific solution only contains one eigenfunction, $n=1$, with wave number given by
\[ k_1 = \frac{1\cdot \pi}{1} = \pi \]

Finally, this gives the following analytical solution of \eqref{eq:diffusion_equation_1D}

\[ u(x,t) = e^{-\pi^2 t} \sin(\pi x) \]

\subsection{Forward Euler unit test results} \label{app:unit_test}
The results of the unit test for the Forward Euler scheme is given in table \ref{tab:unit_test}.  
\begin{table}[h]
	\centering
	\begin{tabular}{l|ll|ll|}
		\cline{2-5}
		& \multicolumn{2}{l|}{\textbf{$j=1$}}                 & \multicolumn{2}{l|}{\textbf{$j=2$}}                  \\ \cline{2-5} 
		& \multicolumn{1}{l|}{\textbf{Manual}} & \textbf{Code} & \multicolumn{1}{l|}{\textbf{Manual}} & \textbf{Code} \\ \hline
		\multicolumn{1}{|l|}{$u_0^j$} & \multicolumn{1}{l|}{0}               & 0             & \multicolumn{1}{l|}{0}               & 0             \\ \hline
		\multicolumn{1}{|l|}{$u_1^j$} & \multicolumn{1}{l|}{0.531656755}     & 0.53165676    & \multicolumn{1}{l|}{0.480888053}     & 0.48088805    \\ \hline
		\multicolumn{1}{|l|}{$u_2^j$} & \multicolumn{1}{l|}{0.8602387}       & 0.8602387     & \multicolumn{1}{l|}{0.778093215}     & 0.77809321    \\ \hline
		\multicolumn{1}{|l|}{$u_3^j$} & \multicolumn{1}{l|}{0.8602387}       & 0.8602387     & \multicolumn{1}{l|}{0.778093215}     & 0.77809321    \\ \hline
		\multicolumn{1}{|l|}{$u_4^j$} & \multicolumn{1}{l|}{0.531656755}     & 0.53165676    & \multicolumn{1}{l|}{0.480888053}     & 0.48088805    \\ \hline
		\multicolumn{1}{|l|}{$u_5^j$} & \multicolumn{1}{l|}{0}               & 0             & \multicolumn{1}{l|}{0}               & 0             \\ \hline
	\end{tabular}
	\caption{Unit test of implementation of Forward Euler scheme. Results from manual calculations are compared with the output of forward euler implementation, for time steps $j=1$ ($t = \Delta t$) and $j=2$ ($t = 2\Delta t$).}
	\label{tab:unit_test}
\end{table}


\bibliographystyle{plain}
\bibliography{refs}
\end{document}

