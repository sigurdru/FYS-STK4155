\documentclass[reprint,english,notitlepage,aps,nobalancelastpage,nofootinbib]{revtex4-1}  % defines the basic parameters of the document
% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage [norsk]{babel} %if you write norwegian
\usepackage[english]{babel}  %if you write english
\usepackage[T1]{fontenc}


%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx, bm}         % include graphics such as plots
\usepackage{upgreek}
\usepackage{float}
\usepackage{xcolor}           % set colors
\usepackage[hidelinks]{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{tikz}             % draw figures manually
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{verbatim}
\usepackage{adjustbox}
\usepackage{mathpazo}
\usepackage{siunitx}
\usepackage{kantlipsum}
\usepackage{bigints}
\usepackage{multirow}
\usepackage{geometry}
\usepackage{algorithm2e}
\usepackage{amsmath}

% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want
\lstset{ %
	inputpath=,
	backgroundcolor=\color{white!88!black},
	basicstyle={\ttfamily\scriptsize},
	commentstyle=\color{magenta},
	language=Python,
	morekeywords={True,False},
	tabsize=4,
	stringstyle=\color{green!55!black},
	frame=single,
	keywordstyle=\color{blue},
	showstringspaces=false,
	columns=fullflexible,
	keepspaces=true}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\ihat}{\boldsymbol{\hat{\textbf{\i}}}}
\newcommand{\jhat}{\boldsymbol{\hat{\textbf{\j}}}}
\newcommand{\khat}{\boldsymbol{\hat{\textbf{k}}}}
\newcommand{\del}[1]{\textbf{#1)}}
\newcommand{\svar}[1]{\underline{\underline{{#1}}}}
\newcommand{\vc}[1]{\mathbf{#1}}
\renewcommand{\deg}{^{\circ}}
\newcommand{\hksqrt}[2][]{\ \mathpalette\DHLhksqrt{[#1]{#2\,}}}
\def\DHLhksqrt#1#2{\setbox0=\hbox{$#1\sqrt#2$}\dimen0=\ht0
	\advance\dimen0-0.3\ht0
	\setbox2=\hbox{\vrule height\ht0 depth -\dimen0}
	{\box0\lower0.65pt\box2}}

\newcommand{\expy}{\mathbb{E}[\mathbf{\tilde{y}}]}
\newcommand{\closed}[1]{\left({#1}\right)}
\newcommand{\bracket}[1]{\left[{#1}\right]}


\graphicspath{{../output/plots/}} % search for figures in this dir



\begin{document}


\begin{titlepage}
	\begin{center}
	\textbf{Analysis of Regression and Resampling Methods}

	\vspace{0.2cm}
	Håkon Olav Torvik, Vetle Vikenes and Sigurd Sørlie Rustad

	\vspace{0.5cm}
	\includegraphics[scale=0.5]{../../pictures/UIO}
	\vspace{0.8cm}

	University of Oslo\\
	Norway\\
	\today	\\
	\end{center}
	\tableofcontents
	\clearpage
\end{titlepage}
%\maketitle                              % creates the title

\onecolumngrid
\section{Introduction}

Regression analysis is a statistical method for fitting a function to data. It is useful for building mathematical models to explain observations. There are several regression methods to achieve this, all with their strengths and weaknesses. We will in this paper study three different methods; ordinary least squares, Ridge and Lasso regression. All the code, results and instructions on running the code can be found in our GitHub repository here\footnote{\href{https://github.com/sigurdru/FYS-STK4155/tree/main/project1}{https://github.com/sigurdru/FYS-STK4155/tree/main/project1}}

Larger datasets contain more information, giving more accurate models. However, real-world datasets usually have a fixed size, and getting more is practically impossible. For smaller datasets it is then useful to have tools mitigating the effects of little data. Resampling methods does this by running the same data through the regression, without over-fitting the model to the sample data. In addition to the regression methods, we will also study the effect of bootstrapping and cross-validating the data.

In order to study this, we need data to analyze. We will use two datasets to study the different methods. One dataset we will generate ourselves using the Franke function, given by equation \eqref{eq:FF}. To make the data more realistic we also add normally distributed noise. The other data is real terrain data from here\footnote{\href{https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2021/Project1/DataFiles}{https://github.com/CompPhysics/MachineLearning
/tree/master/doc/Projects/2021/Project1/DataFiles}}. Our model for both datasets will be a polynomial fit with an $x$ and $y$ dependence of the form $[x,y,x^2,y^2,xy,\dots]$.

\begin{align}
\begin{split}
\label{eq:FF}
f(x,y) = &\frac{3}{4}\exp(-\frac{(9x -2)^2}{4} - \frac{(9y-2)^2}{4}) + \frac{3}{4}\exp(-\frac{(9x + 1)^2}{49} - \frac{(9x + 1)^2}{49} - \frac{(9y + 1)}{10}) \\
& + \frac{1}{2}\exp(-\frac{(9x-7)^2}{4} - \frac{(9y -3)}{4}) - \frac{1}{5}\exp(-(9x-4)^2 - (9y-7)^2)
\end{split}
\end{align}


\section{Ordinary Least Square}

Using the Franke Function (equation \eqref{eq:FF}) we generate our dataset. We pick $n = 100$ randomly distributed points for $x$ and $y$, where our domain is $x,y\in[0,1]$. As mentioned in the introduction we add stochastic noise to the function, using the normal distribution $\epsilon \cdot N(0,1)$. Here $\epsilon$ is just a variable we can use to scale the noise. Choosing $\epsilon = 0.2$, gives us the dataset visualized in figure \ref{fig:FF_dataset}. !!!KOMMENTER SCALING!!!

\begin{figure}[!htb]
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{Frank_anal_eps_0.pdf}
	\endminipage\hfill
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{Frank_anal_eps_0_2.pdf}
	\endminipage
	\caption{Here we have displayed our data along with the analytical function. The left figure shows the analytical Franke function plotted, and the right hand figure is the Franke function plotted with noise, i.e. the data we use.}\label{fig:FF_dataset}
\end{figure}

When we perform ordinary least square (OLS) regression we assume that our data follows a model such that
\begin{equation}
	\vc{z} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{equation}
Where $\vc{z}$ is our data, $X$ is our design matrix given by our model, $\boldsymbol{\beta}$ are parameters in the model and $\boldsymbol{\epsilon}$ is stochastic noise. One challenge in our case is that our data is two-dimensional, i.e. $\vc{z}$ has dimensions $(n,n)$. We create a workaround by mapping our two-dimensional data, to a one dimensional $(n^2,1)$ vector. This is an important note, and we will do this throughout the report. Note that our design matrix $X$ will have dimensions $(n^2,p)$ where $p$ are the number of features.

As mentioned in the introduction our model will be a polynomial fit with an $x$ and $y$ dependence of the form $[x,y,x^2,y^2,xy,\dots]$. This gives us $p = (P + 1)(P + 2)/2$ number of features, where $P$ is our maximum polynomial degree. Now for a given model
\begin{equation*}
	\boldsymbol{\tilde{z}} = X\boldsymbol{\tilde{\beta}},
\end{equation*}
we will have a mean square error (MSE) given by
\begin{equation*}
	C(\boldsymbol{\beta}) = \frac{1}{2N}\sum_{i=0}^{N-1} (z_i - \tilde{z}_i)^2 = \frac{1}{2N}\left\{(\vc{z} - X\boldsymbol{\tilde{\beta}})^T(\vc{z} - X\boldsymbol{\tilde{\beta}})\right\}.
\end{equation*}
We call $C(\boldsymbol{\beta})$ our cost function, and is the quantity we want to minimize. The minima can be found by doing the derivative with respect to all the parameters, and set the result to zero
\begin{equation*}
	\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = X^T(\boldsymbol{z} - X\boldsymbol{\tilde{\beta}}).
\end{equation*}
We can rewrite this as
\begin{equation*}
	X^T\vc{z} = X^TX\boldsymbol{\tilde{\beta}} \implies \boldsymbol{\tilde{\beta}} = (X^TX)^{-1} X^T \vc{z} \equiv \boldsymbol{\hat{\beta}}
\end{equation*}
Thus $\boldsymbol{\hat{\beta}}$ are the parameters that minimizes the cost function. Now that we have an expression for the optimal parameters, we also want to evaluate how good our results are. We do this by splitting our data into two sets, one set for training (80\%) and one set for testing (20\%). We can then find the optimal parameters ($\boldsymbol{\hat{\beta}}$) using the train data, and then compare our test data to the actual data.

There are many quantities we can use to evaluate our results, however we have decided to calculate the MSE and $R^2$ score. We can then plot the quantities as a function of polynomial degree as you can see in figure \ref{fig:OLS_R2_and_MSE}.
\begin{figure}
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{MSE_OLS_n100_eps0_2_pol5.pdf}
	\endminipage\hfill
	\minipage{0.49\textwidth}
	\includegraphics[width=\linewidth]{R2_OLS_n100_eps0_2_pol5.pdf}
	\endminipage
	\caption{Here we have }\label{fig:OLS_R2_and_MSE}
\end{figure}

\section{Bias-variance trade-off and Bootstrapping}

\subsection*{Bias-variance trade-off decomposition}
We assume that our true data is generated from a noisy model with nromally distributed noise $\epsilon$ with a mean of zero and standard deviation $\sigma^2$, i.e.
\begin{align*}
  \mathbf{y} &= f(\mathbf{x}) + \bm{\epsilon}
\end{align*}

We have approximated this function with our design matrix $\mathbf{X}$ and our parameters $\bm{\beta}$ such that our model becomes $\mathbf{\tilde{y}}=\mathbf{X}\bm{\beta}$, where the values of $\bm{\beta}$ were obtained by optimizing the mean squared error via the cost function, given by

\begin{align*}
  C(\mathbf{X}, \bm{\beta}) &= \frac{1}{n} \sum_{i=0}^{n-1} (y_i - \tilde{y}_i) = \mathbb{E}\left[(\mathbf{y} - \mathbf{\tilde{y}})^2\right]
\end{align*}

where $\mathbb{E}$ is the expected value. % Note sample value

We want to show that the above expression can be written as

\begin{align*}
  \mathbb{E}\left[(\mathbf{y} - \mathbf{\tilde{y}})^2\right] &= \frac{1}{n} \sum_i (f_i - \expy)^2 + \frac{1}{n}\sum_i (\tilde{y}_i - \expy )^2 + \sigma^2
\end{align*}

We begin by inserting our model expression for $\mathbf{y}$ and adding and subtracting $\expy$ inside the expected value, before we square the expression.
\begin{align*}
  \mathbb{E}\bracket{(\mathbf{y} - \mathbf{\tilde{y}})^2} &= \mathbb{E}\bracket{(f(\mathbf{x}) + \bm{\epsilon} - \mathbf{\tilde{y}} - \expy + \expy)^2} = \mathbb{E}\bracket{\closed{(f(\mathbf{x}) - \expy) + \bm{\epsilon} + (\expy - \mathbf{\tilde{y}}) }^2 } \\
  &= \mathbb{E}\bracket{(f(\mathbf{x}) - \expy)^2 + \bm{\epsilon}^2 + (\expy - \mathbf{\tilde{y}})^2} \\
  &\quad+ \mathbb{E}\bracket{2\bm{\epsilon} (f(\mathbf{x}) - \expy) + 2\bm{\epsilon}(\expy - \mathbf{\tilde{y}}) + 2 (f(\mathbf{x}) - \expy)(\expy - \mathbf{\tilde{y}})}
\end{align*}

where the cross terms have been written on a separate line since the expected value is linear. Next we will focus on the cross-terms. Since $\bm{\epsilon}$ is normally distributed, it's expected value is simply the mean, which is zero in our case. The two cross terms involving $\bm{\epsilon}$ is therefore zero, so we only need to consider
\begin{align*}
  \mathbb{E}\bracket{(f(\mathbf{x}) - \expy)(\expy - \mathbf{\tilde{y}})} &= \mathbb{E}\bracket{f(\mathbf{x})\expy} - \mathbb{E}\bracket{f(\mathbf{x})\mathbf{\tilde{y}}} - \mathbb{E}\bracket{\expy\expy} + \mathbb{E}\bracket{\mathbf{\tilde{y}}\expy}
\end{align*}
Since the expected value of an expected value is just the expected value itself the last two terms in the above equation both become $\expy^2$, canceling each other out. Using that $f(\mathbf{x})$ is a deterministic function, we have $\mathbb{E}[f(\mathbf{x})]=f(\mathbf{x})$. Expressing $f(\mathbf{x})$ in terms of its expected value, we can write the first two terms in the above equation as
\begin{align*}
  \mathbb{E}\bracket{f(\mathbf{x})\expy} - \mathbb{E}\bracket{f(\mathbf{x})\mathbf{\tilde{y}}} &= \mathbb{E}\bracket{\mathbb{E}\bracket{f(\mathbf{x})}\expy} - \mathbb{E}\bracket{\mathbb{E}\bracket{f(\mathbf{x})}\mathbf{\tilde{y}}} \\
  &= \mathbb{E}\bracket{f(\mathbf{x})}\expy - \mathbb{E}\bracket{f(\mathbf{x})}\expy = 0
\end{align*}

Hence, all the cross terms in the expected value cancel out, and we're left with
\begin{align*}
  \mathbb{E}\left[(\mathbf{y} - \mathbf{\tilde{y}})^2\right] &= \mathbb{E}\bracket{\closed{f(\mathbf{x})-\expy}^2} + \mathbb{E}\bracket{\closed{\expy - \mathbf{\tilde{y}}}^2} + \mathbb{E}\bracket{\bm{\epsilon}^2}
\end{align*}

Using that $\mathbb{E}[\epsilon^2]=\sigma^2$ and writing the expected values as sums we finally arrive at
\begin{align*}
  \mathbb{E}\left[(\mathbf{y} - \mathbf{\tilde{y}})^2\right] &= \frac{1}{n} \sum_i (f_i - \expy)^2 + \frac{1}{n}\sum_i (\tilde{y}_i - \expy )^2 + \sigma^2
\end{align*}
which is what we wanted to show. git

\section{Cross-validation}


\section{Ridge Regression}


\section{Lasso regression}


\section{Analysis of real data}

\section{Testing}
In order to make sure our algorithms are running correctly, it is necessary to perform tests. We did this by comparing our results to those produced by scikit-learn. First of all we generated some simpler data for testing, namely an exponential:
\begin{equation}
	\label{eq:exp}
	f_\text{Test}(x) = exp(x) + \epsilon.
\end{equation}
Here $\epsilon$ denotes normally distributed noise, and $x$ runs from $x_\text{min} = 0$ to $x_\text{max} = 1$ in $N = 50$ randomly distributed steps. This generates the testing data visualized in figure \ref{fig:TestingData}.

\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{Testing_data.pdf}
	\caption{Here you we have plotted the testing data along with the analytical function.}
	\label{fig:TestingData}
\end{figure}

First off we want to test the regression methods we have written. 
\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{Testing_OLS.pdf}
	\caption{A really Awesome Image}
	\label{fig:test_OLS}
\end{figure}
\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{Testing_Lasso.pdf}
	\caption{A really Awesome Image}
	\label{fig:test_Lasso}
\end{figure}
\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{Testing_Ridge.pdf}
	\caption{A really Awesome Image}
	\label{fig:test_ridge}
\end{figure}


\begin{thebibliography}{}
\bibitem[]{ref} Ref.

\end{thebibliography}


\end{document}
